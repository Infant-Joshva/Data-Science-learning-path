{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMQvyKhq0j5eJdPUpcTh7Ih",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Infant-Joshva/Data-Science-learning-path/blob/main/Project_5(AI%20Echo%20-%20Sentiment%20Analysis)/notebook/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing necessary packages"
      ],
      "metadata": {
        "id": "k3sUGY60SYqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Packages for Text preprocessing"
      ],
      "metadata": {
        "id": "ADex8u5dUPB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages for Data Export\n",
        "import pandas as pd\n",
        "# Packages for EDA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "dzjiYoCtH5CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Packages for Text cleaning"
      ],
      "metadata": {
        "id": "wt7k6m-rUIYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing packages for Text cleaning\n",
        "!pip install emoji #installing emojis packages"
      ],
      "metadata": {
        "id": "f0ml-Iq2JOQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re #importing regular expression\n",
        "import emoji #importing emojis packages"
      ],
      "metadata": {
        "id": "y6SRLJnWMArg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Importing packages for Tokenization"
      ],
      "metadata": {
        "id": "cc31UJHgUfJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "frJrmGevM0rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk # Importing needed packages for tokenization\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "V9kP3UTINVPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize # importing packages for word tekenization"
      ],
      "metadata": {
        "id": "n0YACQy9NpjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Importing packages for Normalization"
      ],
      "metadata": {
        "id": "57kdAnKlUqMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "iHxmtsBzNvAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions # Finding the root words"
      ],
      "metadata": {
        "id": "t2iCb9ScN8uB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Importing packages for Lemetization & Stop words"
      ],
      "metadata": {
        "id": "esyZMbANUzmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet') # Download the WordNet corpus for lemmatization\n",
        "nltk.download('omw-1.4') # Download the Open Multilingual WordNet corpus for lemmatization\n",
        "nltk.download('stopwords') # Download the stopwords corpus"
      ],
      "metadata": {
        "id": "QD0DTgVgOG2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer # Import the WordNetLemmatizer for lemmatization\n",
        "from nltk.corpus import stopwords # Import the stopwords corpus"
      ],
      "metadata": {
        "id": "GRBNV0IwOTMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Importing packages for Vectorization"
      ],
      "metadata": {
        "id": "FHuybfpvVKpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer # Doing TF-IDF"
      ],
      "metadata": {
        "id": "zRh-Y5HSVQtb"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exporting data"
      ],
      "metadata": {
        "id": "Yx48KzzvHf4p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15A2pvrWBp5D"
      },
      "outputs": [],
      "source": [
        "sheet_id= '1eyPDJj8ttd8t-o6JVT4txCbvJ9DtcF-U'\n",
        "sheet_name = \"sheet1\"\n",
        "\n",
        "#https://docs.google.com/spreadsheets/d/1eyPDJj8ttd8t-o6JVT4txCbvJ9DtcF-U/edit?gid=1201624046#gid=1201624046\n",
        "\n",
        "url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
        "\n",
        "review_df=pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.head(10)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sR4BcTcVInnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.to_csv('senti_df.csv', index=False)"
      ],
      "metadata": {
        "id": "KnuxYrJyHw66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text preprocessing"
      ],
      "metadata": {
        "id": "XfzzRe0gCCjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.head(10)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "h5e7tEwfIRz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.columns"
      ],
      "metadata": {
        "id": "DTPESBSxJxww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "senti_df = review_df['review']\n",
        "senti_df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GwduDNA_Jqe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling null values"
      ],
      "metadata": {
        "id": "Z0kgg_fOLOZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "senti_df.isnull().sum()"
      ],
      "metadata": {
        "id": "g5mmYOgOLZEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deleting duplicate values"
      ],
      "metadata": {
        "id": "wwTR1vOwLQup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "senti_df.duplicated().sum()"
      ],
      "metadata": {
        "id": "ASFNaJ87KUtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text cleaning"
      ],
      "metadata": {
        "id": "Cnzes7qzJSdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function for Text preprocessing"
      ],
      "metadata": {
        "id": "sNiNt6EgMGpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function for lower case\n",
        "def lowercase(text):\n",
        "  return text.lower()\n",
        "\n",
        "#remove url\n",
        "def remove_urls(text):\n",
        "  urls_pattern=re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "  return urls_pattern.sub(r'',text)\n",
        "\n",
        "# remove Mail\n",
        "def remove_mail(text):\n",
        "  mail_pattern=re.compile(r'\\S+@\\S+')\n",
        "  return mail_pattern.sub(r'',text)\n",
        "\n",
        "# remove HTML tags\n",
        "def remove_html(text):\n",
        "  html_pattern=re.compile('<.*?>')\n",
        "  return html_pattern.sub(r'',text)\n",
        "\n",
        "# remove punctuations\n",
        "def remove_punctuations(text):\n",
        "  punc_pattern=re.compile(r'[^\\w\\s]')\n",
        "  return punc_pattern.sub(r'',text)\n",
        "\n",
        "# remove numbers and number with text\n",
        "def remove_numbers(text):\n",
        "  text=re.sub(r'[A-Za-z]+\\d+','',text)\n",
        "  text=re.sub(r'\\d+\\s*[A-Za-z]+','',text)\n",
        "  text=re.sub(r'\\d+','',text)\n",
        "  return text\n",
        "\n",
        "# remove emoji\n",
        "def remove_emoji(text):\n",
        "  return emoji.replace_emoji(text,replace='')\n",
        "\n",
        "# remove whitespace\n",
        "def remove_whitespace(text):\n",
        "    return re.sub(r'\\s+', ' ', text).strip()"
      ],
      "metadata": {
        "id": "AAZ_8anOMDu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combining all function as a single funcation\n",
        "def text_cleaning(text):\n",
        "  text=lowercase(text)\n",
        "  text=remove_urls(text)\n",
        "  text=remove_mail(text)\n",
        "  text=remove_html(text)\n",
        "  text=remove_punctuations(text)\n",
        "  text=remove_numbers(text)\n",
        "  text=remove_emoji(text)\n",
        "  text=remove_whitespace(text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "CDctE-6mM0WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "VpnlsxofNTDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  return word_tokenize(text)"
      ],
      "metadata": {
        "id": "FzaL6SVPNqCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization"
      ],
      "metadata": {
        "id": "oRqsWZh1Nyl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_contractions(text):\n",
        "  return contractions.fix(text)"
      ],
      "metadata": {
        "id": "CxWzN0C6OAth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemetization & Stop words"
      ],
      "metadata": {
        "id": "tGswL-ZiOIlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lematize and stops words\n",
        "\n",
        "stop_words=set(stopwords.words('english'))\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "def lemmatizer_stopwords(text):\n",
        "  return [lemmatizer.lemmatize(word) for word in text if word not in stop_words]"
      ],
      "metadata": {
        "id": "lnAduDi6PHTZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}